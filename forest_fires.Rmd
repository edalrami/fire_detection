---
title: "Predicting Drought Code Using Weather & Meteorological Data"
author: "Darshil Desai, Edwin Ramirez"
date: "November 27, 2018"
output: 
  html_document:
    toc: true
    toc_float: true
    theme: united
---

# 2.1 Abstract

# 2.2 Introduction
Our project will focus on the forest fires dataset sourced from the UCI Machine Learning Repository. Considering the recent forest fires in the state of California, we beleive this project to be of great relevance as it allows us to harness the power of predictive analytics to forsee the expected forest area to burn given a set of metereological variables

# 2.3 Dataset {.tabset}

## View Dataset

Lets take a look at the features in our dataset

Our dataset comprises of the following 13 variables: 

1 & 2. X,Y : coordinates within the Montesinho park. Ranges from 1 to 9 <br>
3. month: month when the fire frst occured <br>
4: day: day of the given month when the fire occured <br>
5: FFMC (Fine Fuel Moisture Code) : a numeric rating of the moisture content of litter and other cured fine fuels. This code is an indicator of the relative ease of ignition and the flammability of fine fuel. <br>
6: DMC (Duff Moisture Code): A numeric rating of the average moisture content of loosely compacted organic layers of moderate depth. This code gives an indication of fuel consumption in moderate duff layers and medium-size woody material.[^1] <br>
7.Burned area of the forest: area of forest burned  <br>
8. ISI (Initial Spread Index): expected rate of fire spread <br>
9. temperature <br>
10: RH (relative humidity) <br>
11: wind: wind speed <br>
12: rain <br>
13: DC (Drought Code):  A numeric rating of the average moisture content of deep, compact organic layers. This code is a useful indicator of seasonal drought effects on forest fuels and the amount of smoldering in deep duff layers and large logs. 
  This variable will be our response variable and we will try and establish a linear relationship between the myriad of weather and meterological factors of the forest experiencing fires and the future expected area burn. [^2]

``` {r see_data}
#reading in all the packages reqired for the report
library(dummies)
library(ggcorrplot)
library(dplyr)
library(tidyr)
library(ggplot2)
library(corrplot)

#reading in the csv file
dataset <- read.csv('forestfires.csv') 

#displaying first 5 rows
head(dataset)
```

## Brief Summary 
``` {r read_data}
# Lets get a brief summary of our dataset
summary(dataset)
```


## Data Cleaning
Before we pursue our analysis it is vital that we remove, reorganize and convert our data as necessary. We will proceed as follows: 

1. Converting monthly data to seasons: we will go ahead and represent the 12 months into 4 seasons as we deem it necessary to our analysis of seasonal drought. Furthermore, the 4 seasons will then be represented as 4 features comprising binary values indicating what season did the fire occur in 
2. Remove redunant data (PENDING PENDING PENDING)
3. Normalize our dataset. Since our features are on different scales, we believe it neccassary to normalize our features so as to bring them down to one common scale between the range of 0 and 1

``` {r cleaning}

#1  Converting text datetime data to numbers
month_nums <-  c('jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 
                 'nov', 'dec')

# determining Portugal seasons based on the months 
spring <- c('mar', 'apr', 'may')
summer <- c('jun', 'jul', 'aug')
autumn <- c('sep', 'oct','nov')
winter <- c('dec','jan', 'feb')

# initializing empty list to append the season to based on months
seasons <- c()

#looping over the month data in the file and converting them to the appropriate season 
for ( month in dataset$month) {
  if (any(spring==month)) {
    seasons <- c(seasons,'spring')
  } else if (any(summer==month)) {
    seasons <- c(seasons,'summer')
  } else if (any(autumn==month)) {
    seasons <- c(seasons,'autumn')
  } else if (any(winter==month)) {
    seasons <- c(seasons,'winter')
  } 
}  

#attaching the seasons column to the dataset
dataset <- cbind(dataset, seasons)

#checking if seasons and respective months match appropriately
head(dataset$month)
head(seasons)

#2. Removing redundant data
  #1. dropping the original months column
  #2. dropping the "day" column
dataset <- dataset[,-match(c("month","day", 'X', 'Y'),names(dataset))]
dataset1 <- dataset

#Next we need to convert our seasons variables (categorical) into binary representations of the same
dataset <- dummy.data.frame(dataset)
invisible(get.dummy( dataset, 'seasons' ))

#3. Normalizing our dataset
dataset <- as.data.frame(apply(dataset, 2, function(x) (x - min(x))/(max(x)-min(x))))
str(dataset)

# setting a variable to be used later in a plot
plott <- dataset
```

# 2.4 Methodology 

## Correlation with the response variable {.tabset}
We will further our analysis as follows: 
1. In order to create a linear relationship between our response variable (Drought Code) and the other variables we need to first establish a reasonable correlation between our predictor variables and also with the response variable. 

### Correlation Heatmap
Here compare the response variable with the predictor variables. It is vital to establish reasonable collinearity to perform any type of regression analysis. 

``` {r analysis, warning = FALSE}


#1. Visualizing correlations between our response variable and the predictor variable 
# Comparing Y with all other predictor variables *(X's)
corr1<-cor(x = dataset[,-3], y = dataset$DC, use="complete.obs")
ggcorrplot(corr1)

```

### Correlation Scatterplot Matrix

``` {r corr_scatter_mat, warning = FALSE}
dataset %>%
  gather(-DC, key = "var", value = "value") %>% 
  ggplot(aes(x = value, y = DC)) +
    facet_wrap(~ var, scales = "free") +
    geom_point() +
    stat_smooth()

```


## Checking for multicolinearity {.tabset}

### Multicolinearity Heatmap

2. Multicolinearity occurs when predictor variables are linearly correlated with the other. This implies that a change in any one of the predictor variables would entail a change in another highly correlated predictor variable

``` {r m_colin}
#Checking for multicolinearity. Here we will compare each of our predictor variables with all the others predictor variables. 
corr<-cor(dataset)

#plotting correlation head map
corrplot(corr, type = 'upper')
```

### Multicolinearity Scatterplot Matrix

``` {r scat_mat}
# Scatterplot Matrix
pairs(~ dataset$FFMC+ dataset$DMC+ dataset$ISI + dataset$temp
            + dataset$RH + dataset$wind + dataset$rain + dataset$area,data=dataset, 
   main="Simple Scatterplot Matrix")
```

The visual analysis above shows us that there doesn't exist any significant multicolinearity between the predictor variables. In general there always lies the possibility that variables within the same domain will correlate with one another to an extent. However to further confirm our claim, we will employ the use of the Variance Inflation Factor analysis. This analysis allows us to support / reject our claim using numerical proof. 

It is important to note that lower the VIF (lowest being 1), the less multicolinearity exists in our dataset. A VIF of around 5 represents industry standard acceptance rate for multicolinearity as a small value indicates that the standard deviation of the respective variable parameter will remain relatively stable when other predictor variables are added into the regression equation. 

``` {r vif}
#we will go ahead and calculate the Variance Infation Factor for each of our predictor variables
library(car)

model <- lm(dataset$DC ~ dataset$FFMC+ dataset$DMC+ dataset$ISI + dataset$temp
            + dataset$RH + dataset$wind + dataset$rain + dataset$area, data = dataset)
#summary(model)
vif(model)
```

Based on our data above, we can comfirm that there exists no significant multicolinearity in our dataset. Most of our VIF values are under 2.5 far below the industry threshold of 5. 

##  Feature Selection: Stepwise regression
3. We are now equipped with the proper features to proceed with our model. However, in order to derive the most appropriate model we will employ the use of forward step-wise regression. Therefore fitting our model in different ways so as to come up with the best set of features to accurately predict the Drought Code. 


[NOTES FOR EDWIN: 

- do we use the other feature selection methods too?
- 'Seqrep' yields a "linear dependancy"" message. what does that mean? try it out and see!
]

``` {r feature_selection}
#using the leaps package to perform feature selection
library("leaps")

#feature selection: Forward
allSubset = regsubsets(DC ~., data = dataset, method = "forward")
(s = summary(allSubset))
#s1$adjr2
#coef(allSubset, 6)
```

# 2.4 Results & Discussion

## Fitting our model
Based on our selected feature selection process the next step in our analysis will be to construct a model incorporating the selected 8 features. 

``` {r model}
#creating our model
f_model = lm(DC ~ DMC + ISI + temp + RH + wind + seasonsautumn + seasonsspring + seasonswinter, data = dataset)
summary(f_model)

#plotting model results
layout(matrix(1:4,2,2)) 
plot(f_model) 
```

Residual errors plotted versus their fitted values: Ideally we would prefer that residuals should be randomly distributed around the zero line representing a residual error of zero and not show any distinct trend in the distribution of points. 

Considering our anaysis, we see how for the most part the residuals are symetrically and randomly distributed around the zero but however tip very slightly as it further progresses. This slight deviation can be on account of the few outliers residuals. 

Q-Q plot: Ideally the residual errors should be normally distribute. Our model shows similar behavior for the most part. 

##  Visualizing our multi-dimensional model using PCA 
Given that the dataset is multi-dimensional, it is vital to represent it in 2 OR 3 dimensions to comprehend it visually. Therefore our next step is to perform principal component analysis which entails projecting our dataset onto the eigenvectors (principal components) and perform dimension reduction. 

However in order to represent our data in 2D or 3D using 2-3 princiapl components, it is vital to ensure that most of the variance in the data is retained else failing to represent the original data up to satisfaction 

``` {r PCA}
#projecting our data on its principal components

dataset.pca <- prcomp(dataset,
                 center = FALSE,
                 scale. = FALSE) 

#summary
summary(dataset.pca)

#plotting our variances against number of principal components
plot(dataset.pca, type = "l")

pc1 <- c(dataset.pca$x[,1])
pc2 <- c(dataset.pca$x[,2])
```

The plot above compares the variances (y) captured by the principal components with the respective number of principal components (x). Based on the figure above we can conclude that retaining 1 principal component would be moderately appropriate to project on a 3d graph as we retain around 75% of the orginal data
``` {r pca_2d}

#plotting PC1 against the response variable
plot(pc1, dataset$DC, main="Scatterplot", 
  	xlab="Principal Component 1 ", ylab="Drought Code", pch=19)

#plotting the regression line through PC1
abline(lm(DC ~ pc1, data = dataset))

```
The graph above allows us to perform a check for equal variance. We can observe the following: 
1. Our predictor variables (represented through their projection on the first principal component) do not show the ideal equal variance preffered in a regression model. This can be accounted for by the presence of numerous outliers. 


# 2.6 Conclusion

# 2.7
Conclusion
[PENDING]


Footnotes: 

[^1]: http://cwfis.cfs.nrcan.gc.ca/background/summary/fwi <br>
[^2]: http://cwfis.cfs.nrcan.gc.ca/background/summary/fwi 





